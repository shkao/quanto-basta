 I made a single cell guide a little over a year ago, but the field is rapidly changing. There are new recommendations all the time, new tools, etc. So I wanted to make a video covering the most up-to-date workflows and tools, and this time it will also include single cell ATAC in addition to single cell RNA and the integrated analysis of both of them. So this is going to get pretty in-depth. It's going to be a multi-part series, and we're going to get into the nitty-gritty of that analysis. So for this whole series, we're primarily going to be using this longitudinal data set of pediatric AML, and there's a couple reasons. One, because we have a lot of samples. There's 28 different patients, three time points, a lot of cells, and we have RNA and ATAC, not from the same cells, but from the same samples. And I do want to take a moment just to appreciate that these aren't just samples. They came from real people, in this case children who have a terrible disease. So this is a very precious data set in terms of human cost. And it's a pretty in-depth paper. They did a lot of different analyses. We'll recreate some of them and do some new ones that they didn't do. We have a lot to work with. But anyways, let's go to the data availability. We see they have their geolinks. We go to the geopage. Since these are human data, they're not going to have the fast queues except upon request. So let's go ahead and just download the HTTP link right here. So all the RNA data is only about 7 gigs, so it shouldn't take too long. In the meantime, let's get our environment set up. So let's just go ahead and open a new terminal. We'll make it a little bigger. I already have Conda installed, so if you don't, I would recommend using Conda or some other virtual environment manager. So we're going to go ahead and create a new environment. And we're going to specify Python equals to 3.9. Just make sure to activate that. And then we're going to install some of the packages we need for pre-processing. So I'm going to install multiple at the same time. I'm getting attacked right now by my kitty. So I'm just going to install Notebook, ScanP, and doublet detection for now. I have to say install. Now I'm just going to go to my working environment and start Jupyter Notebook. We're going to make a new Python Notebook. And we're just going to import a few modules. OS, ScanP, which we're using to handle the data. Seaborn just to do some plotting. And then Matplotlib just to make a few changes to our plots. And then I'm also going to filter out some warnings just so that my notebook isn't as messy. You don't need to do this. There are a lot of warnings sometimes that don't really mean anything. All right, so our data is done downloading. I'm just going to make a new directory here called data. And then I'm going to move our downloaded data into that. And then we can untar that. Xf I like to remember that as extract the file. It actually moved everything to my current directory. So I'm just going to fix that real quick. We have all the raw data now. And like I said, the reason that I really like this publication and why I picked it is because they upload both the processed and the raw data. So usually when you run CellRanger, people use the filtered data, which includes only the cells that CellRanger called as actual cells. In this case, we have all the droplets, which include mostly empty droplets or even empty droplets, which we'll use to remove background RNA. So we can use OSListr on our data directory to list all of the files and put it in a Python list. So there is one issue with this data set. We're going to have to make a few changes to the feature file, which in this case they call it, what did they call it? Genes. So scanp to read matrices is going to expect this to be called features instead of genes. Let's go back and open another terminal. And let's go into our data directory. We see all these files. Like I said, we have genes and that needs to be features, but we're just going to use rename and it's super simple. We're just going to replace genes with features. Now instead of genes, we have features. So step one, and we have to do one more thing. So now we list this again, we see features instead of genes. So basically there's one more issue with these files. And I'll show you by just getting the top few lines from one of these files, from the feature files. So if we ZCAT one of these feature files and look at the top few lines, we see that there's only two columns and typically cell ranger output has three columns in the feature file, ensemble ID, the gene symbol, and then what is this gene? And typically that says something like gene expression. All we have to do is add another column to these files that says gene expression. I'm going to read them in with Lister. We're only going to modify the feature files so we can filter this with list comprehension. If X has feature in it, so this is going to filter out only the features. Every file name is now a feature file. And we're going to go ahead and make a temp directory here. We're going to use gzip to open them up. So we're reading them in as FN. And then for every one we read N, we're also going to make a temporary output file in our temp directory that we just made. So when we open a file, we're going to loop over all the lines. And for every line, we're going to write that line plus gene expression. So just to double check, we're going to Z cat one of these temp files we just made and see we added gene expression. And I don't want extra files. So I'm just going to move all these temporary files we just made and overwrite the files with the same name and the data directory. So we can actually open these up easily now with scan P. And did I import that earlier? Yes, I did. All right. The first thing we're going to do is remove the ambient RNA. And to do that, we're going to use Cellbender. It's a pretty easy command line tool. We're not going to run it in the notebook. It is a little slower than some of the other ambient removal tools, but it's a good tool by the Broad Institute. It conflicts a lot with other dependencies, so we actually have to make a new Conda environment with it. So we're going to Conda create, I'm just going to call it CD for Cellbender. And importantly, we have to use Python equals 3.7. Activate that. And then we're going to pip install Cellbender. All right. But before we use that, there's one more thing we have to do. So if we look at the input files, there are a couple different formats it inputs. So the H5 file, that's typically in the 10x output, or it also takes H5ad files, so AND data files. Since we have matrix files, we need to create AND data files instead. So there's one more thing we have to do, and that is actually load in all the matrices into ScanP and then save them as an AND data object. And if we were to load in one sample, it would look something like this. So we have to specify a prefix because they're all in the same directory. And if we look at this file in particular, everything before features, even this underscore here is all considered the prefix. So even if there's a bunch of matrices in a folder, we can actually load one individual one. And again, I'm specifying raw instead of processed because I want all of the droplets. You see this is a huge number of cells because most of them are actually empty. These samples, I think, have somewhere between five and 10,000 cells, somewhere in that range. So you know, there's six million some empty droplets here. First, I'm going to make a new directory, raw AData to save all the files. So we have files. It's just a list of all our files. We can wrap this and some list comprehension. So if we were to do extra files, that's just the same thing as files. We can modify X so that it splits each string at underscore raw and then takes the first part. So we're going to take this part here. But like I said, raw is still needed in the prefix. So we'll add it back on. And if we look at that now, we have it split. So I could have gotten rid of this earlier, but we're also going to get rid of anything with processed in the name. So if process not in X, I know this looks a little complicated, but list comprehension is very useful. You should learn it if you don't know it. And then since I only want one prefix per sample and there's three files per sample, I'm just going to get the unique items in this list, which we can do with set. So now we have unique items. So we have that tar file. I'll just get rid of that here too. If process not in X and are not in X. All right. Making this list comprehension as complex as possible. But now we can loop over this. So for prefix and this here, I'll just show you with a friend. Oops. And we fix. So we have all of the individual, I think, 75 different sample prefixes. Let's use those to actually load the samples just like I showed above. But now we're specifying prefix, which we're getting from this loop here. And then after we load the data, we can then save it into the raw data directory, which we just made. And we're going to name it based on the prefix. And it's going to be an H5 AD and data object. All right. So that took about 10 minutes, but now we have our 75 H5 AD files. So just go ahead and open our cell vendor environment. And then we'll just go ahead and go to that raw data directory. So our 75 files. So to run cell vendor on one file would be something like this. So cell vendor, remove, background, input. Then we pass the file. And then the output. Actually before we do that, let's make a new directory. And it's going to be back one directory. And it's going to be clean ADATA, not CD. Let's make it. So our output is going to be in the clean ADATA, whatever our output file name is. I'll just put that there for now. And then we have to include the total droplets included. We're going to set that to 50,000. Off the top of my head, I don't know what the default is. But just of course, the lower the number is, the faster this is going to be. Don't put it anywhere close to how many cells you expect. Do several times higher at least. And then in my case, I have a GPU, specifically an Nvidia GPU and CUDA installed. That will make this a lot faster. You can run it without this on CPU. I don't really know how long that'll take. Cell Bender already isn't the fastest. You might want to try my soup X tutorial in place of this, because soup X is quite a bit faster. This is a lot simpler. We're going to do all of it in one line of code, but soup X is faster. So this is how one file would look. We're going to make it into a loop. Instead of having this file placeholder, we're going to make it a little trickier with base name, just to remove the dot H5 AD. And then the file name instead of H5 AD is going to end in denoised. And then we go here. So for file and our current directory, we could just do that. But just in case there's another file, I'll put H5 AD. So for every file that ends in H5 AD in our current directory, we're going to run cell Bender, remove background, pass that file as input, make an output file in our clean ADATA directory. And we'll see this should run. You have to spell things correct. And I forgot the do as well. So two mistakes. So for file in that do, cell Bender, not cell Ender. All right, so this, I think I have a 4090. So this is going to take about 20 minutes, probably, per sample. And there are 75 samples by the next day. Hopefully, it'll be done. All right, so when that's done, you're going to have a lot of files in your clean ADATA directory. So we had 75 ADATA objects. Now we have 600 files because it makes eight files. So there's a couple of things we can look at here. One is these QC plots. So basically, what you're looking for here is a steep increase, either a plateau or like a gradual increase over time. But what you don't want to see is if this goes up and down a lot, that means there was probably issues with the training. And then on top of removing the background, cell Bender also determines which cells or which droplets are actually cells. So if you look at this plot, we have the UMI counts and then the probability it's a cell. So of course, the droplets with the most UMI are going to have high probabilities of being a cell. And we don't have to really worry about that. So you can just quickly go through and check the different training plots just to double check that they all look OK. But for the most part, cell Bender usually does a pretty good job. So back to our notebook, we can list our clean ADATA directory. We want the ones that end in filtered H5. So these are the cells that cell Bender calls. We also have this metrics file. So let's actually see how much background RNA was removed. So instead of going through this line by line, I'm just going to copy what I did over here. So I'm going through all these files and only getting the metrics CSV, opening them up, adding them to this metrics list and then using pandas. I might not have pandas imported, so let's do that. But we have this metrics data frame, one row for each sample, and we have fraction of counts removed. So they're all in like 1.5, 1.3, 2 percent range. This one is about 6. So we wanted to, we could just plot a quick histogram of that. And you see they're most around 2 percent or so with a few outliers. As long as it's not about like 10 percent, it's pretty expected. So like I said, let's actually get these filtered H5 files now. So we're going to go ahead and make a list called ADATAS. We're going to list all the files and get the files that only end with filtered. So we have only 75 H5 files in this list now, one for each sample. OK, so we need to actually load these in. In addition to loading them in, I'm going to use the names to get the metadata. So we have our patient ID and the diagnosis. So again, there's the remission, diagnosis and relapse. So three for each sample. And I'm just going to make a function called load it. We're going to pass one of these strings. We're going to split it by the underscore and then get the second item. So sample or the patient ID going to get the third item. So the diagnosis, we're actually going to pass the path between ADATA, where these files are saved, plus the file name. And then all we're going to do is open it, then add three columns into the observation data frame, patient diagnosis, and then to differentiate between the individual samples, even within the patient, we're going to make a sample column, which is the patient plus the diagnosis. And then I'm also going to modify the cell barcode to include the sample and diagnosis, just so that when we eventually concatenate the samples, there's no, so we can convert our list of strings here and to an actual list of and data. If you don't have that much RAM, I would recommend maybe doing fewer samples. You can just, I don't know, do something like this and do like the first 25 samples instead of all 75. For this computer that I use to record videos, I only have 128 gigs of RAM. And with these 75 samples, at least now with RNA, I might not run into issues, but when we start doing ATAC analysis too, it might get a little tricky. And I'll load this in and tell you how much RAM it actually takes. So it's only using 25 gigs of RAM right now. It will use more, especially when we add more to the objects. And then if any function requires a sparse matrix to become a dense matrix, it'll increase the memory at least by like two fold or something. So, but now we have our list of and data with all this work. We've only really done two things, get rid of background RNA, ambient RNA, and used cell bender to determine what's an actual cell. But, you know, cell bender, while it does pretty good job at removing background RNA, it doesn't actually, in my experience, do that well determining what a cell is. It seems to include a lot of really low UMI count cells sometimes. So we're expecting around 10,000, but you'll see every once in a while, we'll get some with around 30,000. So we're still going to have to filter that out. We're going to make a QC function here. It's not going to be a filter function. We are going to filter out anything below 200, because anything below 200 is really not going to be cell, unless your specific data set has really low cell count overall. Typically 200 is a pretty good lower level threshold. We're going to annotate the mitochondrial genes. Typically when you're working with human data, the MT is going to be capitalized. So if we look at ADATA or the VAR names, so the actual gene names, they're all capitalized. Mouse, typically the first letter is capital, then the rest are lowercase. So if we do this and if we wanted to see the actual VAR names, here are our mitochondrial genes. They all start with MT dash. We're just labeling true or false in the ADATA VAR data frame. It's going to add another column. And then we'll do the same thing with ribosomal genes, which typically start with RPS and RPL. Again, you'll have to fix that if this is mouse data that you're working with. And then we can also label hemoglobin. Arithrocytes will have a high amount of hemoglobin, and it can also be indicative of stress response sometimes, or technical artifacts, so it's hard to say. But we'll annotate it. You can actually use these variables to correct some of your data. So we'll annotate the VAR data frame and then use that VAR data frame we annotated to calculate these QC metrics along with some other standard single cell QC metrics. There's going to be a lot of extra columns, so I like to remove some that we're never going to use. So we're getting rid of columns in ADATA OBS columns if they're in this remove list. And then we're returning ADATA. So just a simple function, not really doing anything too crazy other than adding some QC metrics to our data. And then we'll pass our ADATA's list and QC every object in it. Okay, so now that those are QC'd, we're actually going to pull out all the QC metrics from each ADATA object and put them all into one data frame just so it's easy to graph all of the samples at the same time. If you have only a few samples, you don't have to put them all together. You can look at them individually. But in this case, I don't really want to do that with 75 samples. So I'm just going to make a data frame concatenation of all the different OBS data frames from every scanP object in our ADATA's list. So we just have this big data frame now with 420,000 cells. And then I'm just going to sort these by sample so that when we plot it, it'll be in the right order. So we have this big plotting function. I'm not going to go through it in detail. You can just copy and paste it. You might have to change a few things depending on your data. Like the name of the columns. In this case, we're using sample. But basically, I'm making a bunch of ridge plots here. So you get a lot of these spaces. But making a bunch of ridge plots, you'll see. So the first one I'm going to do is percent counts mitochondrial. There are 75 samples, so it might take 30 seconds or so to actually process. All right. Once that's done, we have the distribution of percent mitochondrial counts for each of the samples. And if we scroll down all the way to the bottom, here's the actual x-axis. I plotted also this red line is the median. So we see none of the samples. There is some variation, but none of them are huge outliers. They all fall around 0% to 20% at max. Some of these must have gone all the way close to 100. That's why our axis goes all the way to 100. But in terms of the distribution, they're not that many. And those will get filtered out when we do the actual pre-processing steps. You can go ahead and look at all of these different values, like the number of genes. The reason I included these is because we're going to be using these to do the filtering. So if we look at the number of genes. So here we can see some of them have these multimodal distributions, which, for example, here these might get filtered out. They might not be actual cells. But the median around 2000, there is some variation between samples. It's hard to say without knowing more about processing why that is. Typically, less variation is a good thing. And then after we do pre-processing, you'll see all these will tighten up a little bit. It does change depending on the cell type. So some cells have more genes. Some cells have fewer genes. But, you know, that 2000 range is pretty good. So let's go ahead and look at the percent counts in the top 20 genes. So there's always a few genes that are highly expressed all the time. And they do account for a big proportion of the total reads. There are some workflows actually in the processing end where people try to deplete these genes so that you get more informative genes. On the analysis end, you know, you're stuck with this. At this point, they're already there. But it is a good QC metric. So we'll check that out. So again, some variation. If we scroll through, there are some multimodal distributions. This is a good QC metric and a good metric to filter out some of these lower quality cells. But you see just 20 genes account for around, on average, like 20 to 30 percent of the total counts from a cell, even though in each cell there's around 2000 to 3000 genes. And then finally, if we look at the total counts, but log one piece scale, some of these have these multimodal distributions, like this population of cells that has a really low count. It's going to get filtered out later when we do our pre-processing. So in our pre-processing, we're going to do a few different things. We're going to filter out these low quality cells based on the QC metrics. And then we're also going to get rid of doublets. So we're going to use doublet detection to do the doublet removal. I like this just because it's super easy to use and it's pretty fast. There are other methods, and actually a good way is to use multiple different tools to detect doublets. And you can take the union of those because they're all going to give slightly different output. And to which one is the best, I'm not sure I've come across a review that does in-depth comparison between the two. So there's a bunch of different tools here. I'm just picking doublet detection because it's very easy to use. If you want to look at another method that I like a lot, I like to use a CVI, specifically the solo extension to remove doublets. I have that in my previous single cell tutorial. And then we're going to use median absolute deviations. And I'll show an example of that in a second. And then we also need a numpy. Let's actually use one of these samples. We're going to look at this one specific sample and we're going to get the log1p total counts. We can get the median from this list of numbers. This is just, it's just all those log1p total counts from this one sample. And then we can get five times the median absolute deviation of this distribution below the median. So the median's 8.89. Five deviations below is 6.3. And then likewise, five medians above. If we change this to plus is 11.4. So let me just quickly show if we plot this distribution. So I'm just a disk plot. And then I'm going to plot these two values on to the plot. Here we see that all these cells between the two lines are within five median absolute deviations. So we're going to be using this concept actually to do a lot of the filtering. You could manually go through and set different thresholds. Like, of course, this one might be slightly better if it was right about here, but we have 75 samples and this has been proposed as a pretty good method to do this kind of pre-processing. So to do this, we're going to write a simple little helper function here. And I'm just going to call it mad outlier. So we're going to pass a couple things to this function. The a data object, the metric. So we're going to pull in this case, the metric was log one P total counts, but we can specify to pull that from the observation data frame. The number of meeting absolute deviations because some metrics need five, some like mitochondrial need three. I also include this upper only because for some of these distributions, we don't want to remove cells with low counts. For example, mitochondrial, we don't want to remove cells with low mitochondrial counts just because they vary from the distribution. So if we pass true for upper only, we're only going to get rid of the ones that are five or whatever median deviations above the mean median. Sorry. All this function is doing is if they're outside these boundaries, it's going to return true. And if it's true, we'll know later on we can filter those cells out. And we're also going to use this doublet detection. Like I was talking about, these are the recommended settings for the doublet detection, but anyway, we can make our pre-processing function. Now it's going to take one of these and data objects, which is going to come from our list and we're start by getting rid of anything that is above 25% because anything above that is likely going to be dead or dying. So now we get a bool vector. We're basically passing each metric. So all four of those metrics I showed earlier, the number of deviations, they're allowed to vary each individual function returns a list of true or false. So you can actually add these lists like this. If two of the same cells are true, it'll output true. If some are false and one is true, it'll still output true. And if they're all false, it's going to output false for that cell. So if they return true for any one of these metrics, that cell will be labeled true. And then to filter out all that are true. I know it looks like we're adding a lot of things, but it's just going to return one list of true or false. So we can pass that to our and data object. And then if we put this tilde here, that means keep only the ones that are false. And then we're going to determine the doublets. So we're using that CLF that we initialize up here to fit the and data, the counts, and then use that to predict if it's going to be a doublet or not. And then we can get the doublet score. So these values we can change to make the algorithm more conservative or not. These are the recommended thresholds. So let's see how these do. And then we can play around to see if we should change them. We can add a new column, whether it's a doublet or not. And that's just going to be one or zero. One, if it is a doublet, zero, if it's not a doublet. And then the score. And then just for keeping track of some of this, because we're applying it to a lot of different samples, I'm going to add one of these variables in the unstructured part of the and data object. And it's just going to be a sum of doublet. And like I just said, doublets one or zero. So it's going to be the number of total doublets detected. And then I'm only going to keep a cell if doublet is equal to zero. And then finally, we can actually return that data. So not too complicated. Getting rid of outlier cells and then getting rid of doublets. Actually, now that I'm looking at this, I also want to add just a number of cells filtered. Just to keep track. And that's going to be I think we can just sum this full vector. Because true is typically equal to one. So let's see if that works. Hopefully it doesn't give me an error. Now we can pre-process all our and data objects that we've already QC'd. It's actually showing the progress of each individual sample. It'll take a little while and then we can come back and look at how it did. But let's check how that actually turned out. So I'm just going to for AData and AData's going to print. So what did we save this? We saved it as cell remove and doublet's removed. Just print those. We have some variation here, but let's actually print the length of the AData. So we would expect more than seven doublets, for example, and an AData object with 12,000 cells. So I might actually change these parameters, the default or the recommended parameters for the doublet removal. But let's take a look at a couple of these. Yeah, so seven doublets. We did filter out a lot of cells, but only seven for 12,000. That's not that many. Zero, so smaller data set. 1,300, but still pretty low. Yeah, there's quite a bit of variation. Only 79 cells in this AData object. Apparently I might want to take a look at what happened there. Yeah, I think I'm going to take a look at that. Not right now, but you should verify that this AData object should only have 79 cells. But anyway, so the number of doublets we expect, I don't know if you can see this kind of low res image, but for around 5,000 ish cells, we expect a doublet rate of around four and a half percent. So when you see zero cells, it is a little concerning. I am going to drop this value so you can either increase this or make it closer to one or decrease this closer to zero. I do know that you can drop it to three instead of 16, and that's going to give you more doublets detected. It's not going to be a drastic number. It's not going to be 13 magnitudes different. It's only going to be like double maybe, but we're going to go ahead and run that again. I know I just wasted 20 minutes, but let's go ahead and start from here just in case. So we turn them back into list. You see skip graphing them again. And then just run that again. And 20 minutes, we'll come back and see how that turned out. So that ran again. I looked at the cell numbers this time. We do catch a lot more doublets. It's not a huge, huge difference on this one sample that was like 12,007 doublets. We do catch a lot more. This is really high. This is like a 30% doublet rate if you include the filtered and then these. It's better to catch more than not enough so you don't have these cell types that seem like new cell types just because they're actually a combination of multiple cells. So I did also during that check this sample here. If we look at the actual processed barcode list for this sample, so what did cell ranger actually do? So what did cell ranger actually call a cell from this sample? And look at how many there are. There's only 36. So it's not something that we did wrong in our pre-processing. It's just that there is some variability in these samples. And it's hard to say. I'm not the one that collected them. So it's hard to say where this variability comes from. It might have been unavoidable based on sample collection, et cetera. But the pre-processing is fine. So we didn't do anything wrong. If you do see really big outliers here, it is kind of good to start from the beginning and see why that is. I'm not going to go through more than just this just for time's sake. You want to make sure the pre-processing is right because it'll affect everything downstream. And I do want to point out that single cell best practices by the Tice Lab is a great resource. They have a lot of good information and they go into depth about why we're doing some of these steps. Some of these things I can't really fit in one video. So now that pre-processing is done, we will go to cell annotation next and then get into some of these downstream analyses.

Source URL: https://www.youtube.com/watch?v=cmOlCTGX4Ik